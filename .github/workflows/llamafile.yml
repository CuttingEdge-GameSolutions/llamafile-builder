name: Build llamafile

on:
    workflow_dispatch:
        inputs:
            model_url:
                description: "URL to the model to be used"
                required: true

            huggingface_repo:
                description: "HuggingFace repo to upload to. Example: rabil/llamafile. It is assumed that you have added your HuggingFace token with write access to your action secrets."
                required: true


jobs:
    build:
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@v4
              with:
                repository: "Mozilla-Ocho/llamafile"
                ref: "main"
                path: "llamafile"

            - uses: actions/checkout@v4
              with:
                path: "llamafile-builder"

            # Add build cache
            - uses: actions/cache@v3
              with:
                path: |
                  ~/.cache/
                key: ${{ runner.os }}-cache
                restore-keys: |
                  ${{ runner.os }}-

            - name: llamafile gotchas error
              run: |
                sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf
                sudo chmod +x /usr/bin/ape
                sudo sh -c "echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register"
                sudo sh -c "echo ':APE-jart:M::jartsr::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register"

            - name: Install GCC and Make
              run: sudo apt-get install make gcc g++ -y

            - name: Prepare build
              run: |
                cd llamafile
                make -j$(nproc)
                sudo make install PREFIX=/usr/local

            - name: Find model name
              id: model_name
              run: |
                modeln=$(echo ${{ github.event.inputs.model_url }} | grep -o '[^/]*$' | sed 's/?download=true//')
                echo "model=$modeln" >> $GITHUB_OUTPUT

            - name: Download model
              run: |
                wget -O ${{ steps.model_name.outputs.model }} ${{ github.event.inputs.model_url }}

            - name: Create .args
              run: |
                cd llamafile
                echo "-m" > .args
                echo "${{ steps.model_name.outputs.model }}" >> .args
                echo "--host" >> .args
                echo "0.0.0.0" >> .args
                echo "..." >> .args

            - name: Prepare llama-server
              run: |
                # remove extension from model name
                cd llamafile
                file_ext_removed_model_name=$(echo ${{ steps.model_name.outputs.model }} | sed 's/\.[^.]*$//')
                cp /usr/local/bin/llamafile-server "$file_ext_removed_model_name".llamafile
                zipalign -j0 "$file_ext_removed_model_name".llamafile ${{ steps.model_name.outputs.model }} .args

            - name: Setup python
              uses: actions/setup-python@v5
              with:
                python-version: "3.11"
                cache: "pip"

            - name: Install dependencies
              run: |
                python -m pip install huggingface_hub

            - name: Upload to HuggingFace
              env:
                HF_TOKEN: ${{ secrets.HF_TOKEN }}
              run: |
                cd llamafile-builder
                file_ext_removed_model_name=$(echo ${{ steps.model_name.outputs.model }} | sed 's/\.[^.]*$//')
                python3 hf.py ${{ github.event.inputs.huggingface_repo }} "$file_ext_removed_model_name".llamafile


            

            

            
